{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2edc36-8cc7-4d64-8083-c8be0544c3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# W261 Final Project - Experiment, Fine-Tune, Select the Optimal Pipeline (Coding Notebook - Data Preprocessing + EDA + Dimensionality Reduction - 5 Year)\n",
    "## Section 02, Team 1: Aimee, Dylan, Jo, Vicky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e31cc1e-bd39-4cfa-9a39-353dbd47c8c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Load for the Project Phase 3\n",
    "\n",
    "OTPW Data (five year dataset): This is our joined data (We joined Airlines and Weather), the main dataset for the project. Location `dbfs:/mnt/mids-w261/OTPW_60M/OTPW_60M/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071c66bd-a612-4987-8335-85c30c7f5c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check the folder path to load data\n",
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/OTPW_60M/OTPW_60M/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d43b5d5-5afa-4b41-904c-c19aa2b079e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read datasets\n",
    "section = \"02\"\n",
    "number = \"01\"\n",
    "folder_path = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "\n",
    "df_otpw_60m = spark.read.parquet(f\"{folder_path}/otpw_60m.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc35887-1fb9-4de3-b12e-1a70031c19ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_otpw_60m.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d058f4-3eff-406e-84b7-dfd36d210ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df_otpw_60m.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb6303b-7858-4b9d-b883-fe307b2b29c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EDA on OTPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93277668-4960-4505-ba1e-a921a4b94032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_otpw_60m.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec949f2f-7e27-4a90-9e3e-6e3e3a2b69e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabb308d-4040-44c0-a3af-526dcc98590a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# sort by station and date and select the column names that contain \"Hourly\"\n",
    "display(df_otpw_60m.sort(col(\"STATION\"), col(\"DATE\")).select(['DATE', 'STATION']+[col for col in df_otpw_60m.columns if \"Hourly\" in col]))\n",
    "\n",
    "# Ensure DATE is a timestamp\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"DATE\", F.col(\"DATE\").cast(\"timestamp\"))\n",
    "\n",
    "# Define window partitioned by STATION and ordered by DATE\n",
    "window_spec = Window.partitionBy(\"STATION\").orderBy(\"DATE\")\n",
    "\n",
    "for col_name in [col for col in df_otpw_60m.columns if \"Hourly\" in col]:\n",
    "    # Get the previous timestamp and value within each station\n",
    "    prev_time = F.lag(\"DATE\").over(window_spec)\n",
    "    prev_value = F.lag(col_name).over(window_spec)\n",
    "\n",
    "    # Calculate time difference in hours\n",
    "    time_diff = (F.unix_timestamp(\"DATE\") - F.unix_timestamp(prev_time)) / 3600\n",
    "\n",
    "    # Forward fill only if the previous timestamp is within 6 hours\n",
    "    df_otpw_60m = df_otpw_60m.withColumn(\n",
    "        col_name,\n",
    "        F.when(time_diff <= 6, F.coalesce(F.col(col_name), prev_value)).otherwise(F.col(col_name))\n",
    "    )\n",
    "\n",
    "# Show the updated DataFrame\n",
    "display(df_otpw_60m.sort(F.col(\"STATION\"), F.col(\"DATE\")).select(['DATE', 'STATION']+[col for col in df_otpw_60m.columns if \"Hourly\" in col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988402e3-50a0-430a-9816-f16d9013d29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m = df_otpw_60m.withColumn(\"HourlyPrecipitation\", F.when(F.col(\"HourlyPrecipitation\") == \"T\", \"0.001\").otherwise(F.col(\"HourlyPrecipitation\")))\n",
    "\n",
    "# cast HourlyPrecipitation to double\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"HourlyPrecipitation\", F.col(\"HourlyPrecipitation\").cast(\"double\"))\n",
    "\n",
    "print(\"HourlyPrecipitation null counts\",df_otpw_60m.filter(F.col(\"HourlyPrecipitation\").isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aaf137d-5c7d-49f0-a422-595e208d6632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For the HourlyPressureChange Column, remove the \"+\" and cast as a double\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"HourlyPressureChange\", F.regexp_replace(\"HourlyPressureChange\", \"\\+\", \"\").cast(\"double\"))\n",
    "\n",
    "\n",
    "# get unique values in the HourlyPressureChange column s\n",
    "display(df_otpw_60m.select(\"HourlyPressureChange\").distinct())\n",
    "\n",
    "df_otpw_60m.filter(F.col(\"HourlyPressureChange\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ccbe2b9-768a-45e1-a0f3-52fd92a09c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a window partitioned by ORIGIN_CITY_MARKET_ID over historical data\n",
    "window_spec = Window.partitionBy(\"ORIGIN_CITY_MARKET_ID\")\n",
    "\n",
    "hourly_cols_to_fill = [\"HourlyPrecipitation\", \"HourlySeaLevelPressure\", \"HourlyWindGustSpeed\", \"HourlyPressureChange\", \"HourlyDewPointTemperature\", \"HourlyDryBulbTemperature\", \"HourlyStationPressure\", \"HourlyVisibility\",'HourlyWetBulbTemperature','HourlyWindSpeed']\n",
    "\n",
    "\n",
    "for column in hourly_cols_to_fill:#['HourlyPrecipitation','HourlySeaLevelPressure','HourlyWindGustSpeed','HourlyPressureChange']:\n",
    "    df_otpw_60m = df_otpw_60m.withColumn(column, F.col(column).cast(\"double\"))\n",
    "\n",
    "    # Compute the average HourlyPrecipitation for each ORIGIN based on data before 2019\n",
    "    df_otpw_60m = df_otpw_60m.withColumn(\n",
    "        \"avg_\" + column,\n",
    "        F.avg(F.when(F.col(\"DATE\") < \"2019-01-01 00:00:00\", F.col(column)))\n",
    "        .over(window_spec)\n",
    "    )\n",
    "\n",
    "    # Fill null values in the later dataset (DATE >= \"2019-01-01\") with the computed average\n",
    "    df_otpw_60m = df_otpw_60m.withColumn(\n",
    "        column,\n",
    "        F.when(\n",
    "            (F.col(\"DATE\") >= \"2019-01-01 00:00:00\") & F.col(column).isNull(),\n",
    "            F.col(\"avg_\" + column)\n",
    "        ).otherwise(F.col(column))\n",
    "    )\n",
    "\n",
    "    # Drop the helper column\n",
    "    df_otpw_60m = df_otpw_60m.drop(\"avg_\" + column)\n",
    "\n",
    "# Calculate the average values for the columns\n",
    "avg_hourly_precipitation = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyPrecipitation\")).first()[0]\n",
    "avg_hourly_sea_level_pressure = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlySeaLevelPressure\")).first()[0]\n",
    "avg_hourly_wind_gust_speed = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyWindGustSpeed\")).first()[0]\n",
    "avg_hourly_pressure_change = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyPressureChange\")).first()[0]\n",
    "avg_hourly_dew_point_temp = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyDewPointTemperature\")).first()[0]\n",
    "avg_hourly_dry_bulb_temp = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyDryBulbTemperature\")).first()[0]\n",
    "avg_hourly_station_pressure = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyStationPressure\")).first()[0]\n",
    "avg_hourly_visibility = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyVisibility\")).first()[0]\n",
    "avg_hourly_wet_bulb_temp = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyWetBulbTemperature\")).first()[0]\n",
    "avg_hourly_wind_speed = df_otpw_60m.filter(col(\"DATE\") < \"2019-01-01 00:00:00\").select(F.avg(\"HourlyWindSpeed\")).first()[0]\n",
    "\n",
    "\n",
    "\n",
    "# Fill in null values with the calculated averages\n",
    "df_otpw_60m = df_otpw_60m.fillna(\n",
    "    {\n",
    "        'HourlyPrecipitation': avg_hourly_precipitation,\n",
    "        'HourlySeaLevelPressure': avg_hourly_sea_level_pressure,\n",
    "        'HourlyWindGustSpeed': avg_hourly_wind_gust_speed,\n",
    "        'HourlyPressureChange': avg_hourly_pressure_change,\n",
    "        'HourlyDewPointTemperature': avg_hourly_dew_point_temp,\n",
    "        'HourlyDryBulbTemperature': avg_hourly_dry_bulb_temp,\n",
    "        'HourlyStationPressure': avg_hourly_station_pressure,\n",
    "        'HourlyVisibility': avg_hourly_visibility,\n",
    "        'HourlyWetBulbTemperature': avg_hourly_wet_bulb_temp,\n",
    "        'HourlyWindSpeed': avg_hourly_wind_speed\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# print the number of null values in the HourlyPrecipitation Column\n",
    "print(\"HourlyPrecipitation null counts\",df_otpw_60m.filter(F.col(\"HourlyPrecipitation\").isNull()).count())\n",
    "print(\"HourlySeaLevelPressure null counts\",df_otpw_60m.filter(F.col(\"HourlySeaLevelPressure\").isNull()).count())\n",
    "print(\"HourlyWindGustSpeed null counts\",df_otpw_60m.filter(F.col(\"HourlyWindGustSpeed\").isNull()).count())\n",
    "print(\"HourlyPressureChange null counts\",df_otpw_60m.filter(F.col(\"HourlyPressureChange\").isNull()).count())\n",
    "print(\"HourlyDewPointTemperature null counts\",df_otpw_60m.filter(F.col(\"HourlyDewPointTemperature\").isNull()).count())\n",
    "print(\"HourlyDryBulbTemperature null counts\",df_otpw_60m.filter(F.col(\"HourlyDryBulbTemperature\").isNull()).count())\n",
    "print(\"HourlyStationPressure null counts\",df_otpw_60m.filter(F.col(\"HourlyStationPressure\").isNull()).count())\n",
    "print(\"HourlyVisibility null counts\",df_otpw_60m.filter(F.col(\"HourlyVisibility\").isNull()).count())\n",
    "print(\"HourlyWetBulbTemperature null counts\",df_otpw_60m.filter(F.col(\"HourlyWetBulbTemperature\").isNull()).count())\n",
    "print(\"HourlyWindSpeed null counts\",df_otpw_60m.filter(F.col(\"HourlyWindSpeed\").isNull()).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01354636-a646-48c7-b22d-fd683cc5cd56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get unique values in the HourlyPressureTendency column\n",
    "df_otpw_60m.select(\"HourlyPressureTendency\").distinct().show()\n",
    "\n",
    "# count null values in the HourlyPressureTendency column\n",
    "df_otpw_60m.filter(F.col(\"HourlyPressureTendency\").isNull()).select(\"HourlyPressureTendency\").count()\n",
    "\n",
    "# fill null values with -1 as an indicator that the value is not available\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"HourlyPressureTendency\", F.when(F.col(\"HourlyPressureTendency\").isNull(), -1).otherwise(F.col(\"HourlyPressureTendency\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f40dfa8-ee72-4d04-8014-539f18b4fe71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data description 2.- Data size and source.\n",
    "print(f\"There are {df_otpw_60m.count()} rows and {len(df_otpw_60m.columns)} columns in our dataset\")\n",
    "\n",
    "# data source https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14158c3-9ae9-49fc-a244-6270f6c8966b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count null values per column - DS\n",
    "null_counts = {column: df_otpw_60m.filter(F.col(column).isNull()).select(column).count() for column in df_otpw_60m.columns}\n",
    "null_counts_df = pd.DataFrame(list(null_counts.items()), columns=['column_name', 'null_count'])\n",
    "null_counts_df = null_counts_df.sort_values(by=['null_count'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12da664-3cbe-4b52-93a7-2260f9f2f13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1,figsize=(15,10))\n",
    "\n",
    "threshold = 0.5 * df_otpw_60m.count()\n",
    "ax[0].bar(null_counts_df['column_name'], null_counts_df['null_count'])\n",
    "ax[0].axhline(y=round(threshold,0), color='r', linestyle='--', label=f'Threshold = {round(threshold,0)}')\n",
    "ax[0].set_xlabel('Column')\n",
    "ax[0].set_ylabel('Null Count')\n",
    "ax[0].set_title('Null Counts per Column (60m)')\n",
    "ax[0].set_xticks([])  # Remove x-axis ticks\n",
    "ax[0].legend()\n",
    "ax[0].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Remove columns with more than 80% null values\n",
    "cols_to_drop = [c for c in df_otpw_60m.columns if df_otpw_60m.filter(F.col(c).isNull()).count() > threshold]\n",
    "df_otpw_60m = df_otpw_60m.drop(*cols_to_drop)\n",
    "\n",
    "# recalc null counts\n",
    "null_counts = {column: df_otpw_60m.filter(F.col(column).isNull()).select(column).count() for column in df_otpw_60m.columns}\n",
    "null_counts_df = pd.DataFrame(list(null_counts.items()), columns=['column_name', 'null_count'])\n",
    "null_counts_df = null_counts_df.sort_values(by=['null_count'],ascending=False)\n",
    "\n",
    "ax[1].bar(null_counts_df['column_name'], null_counts_df['null_count'])\n",
    "ax[1].axhline(y=round(threshold,0), color='r', linestyle='--', label=f'Threshold = {round(threshold,0)}')\n",
    "ax[1].set_xlabel('Column')\n",
    "ax[1].set_ylabel('Null Count')\n",
    "ax[1].set_title('Null Counts per Column after Dropping Columns (60m)')\n",
    "ax[1].set_xticks([])  # Remove x-axis ticks\n",
    "ax[1].legend()\n",
    "ax[1].ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7b16d7-81b0-4bbd-8f81-74b0a495d249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DS Data Cleaning and Preprocessing\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "\n",
    "# List of columns to cast to double\n",
    "cast_cols_to_double = [\"DEP_DELAY\", \"DEP_DELAY_NEW\", \"DEP_DELAY_GROUP\", \"DISTANCE\", \"AIR_TIME\",\n",
    "                   \"origin_station_lat\",\"origin_station_lon\",\"origin_airport_lat\",\"origin_airport_lon\",\"origin_station_dis\",\n",
    "                   \"dest_station_lat\",\"dest_station_lon\",\"dest_airport_lat\",\"dest_airport_lon\",\"dest_station_dis\",\n",
    "                   \"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"HourlyAltimeterSetting\",\"HourlyDewPointTemperature\",\"HourlyDryBulbTemperature\",\"HourlyPrecipitation\",\n",
    "                   \"HourlyPressureTendency\",\"HourlyRelativeHumidity\",\"HourlySeaLevelPressure\",\"HourlyStationPressure\",\"HourlyVisibility\",\t\"HourlyWetBulbTemperature\",\"HourlyWindDirection\",\"HourlyWindSpeed\",\"HourlyWindGustSpeed\"]\n",
    "\n",
    "# Cast columns to double\n",
    "for col in cast_cols_to_double:\n",
    "    if col in df_otpw_60m.columns:\n",
    "        df_otpw_60m = df_otpw_60m.withColumn(col, F.col(col).cast(\"double\"))\n",
    "\n",
    "# Cast other columns to appropriate data types\n",
    "\n",
    "cast_cols_to_timestamp = [\"FL_DATE\",\"sched_depart_date_time\",\"sched_depart_date_time_UTC\",\n",
    "                           \"four_hours_prior_depart_UTC\",\"two_hours_prior_depart_UTC\",\"DATE\",\"WindEquipmentChangeDate\"]\n",
    "\n",
    "# Cast columns to timestamp\n",
    "for col in cast_cols_to_timestamp:\n",
    "    if col in df_otpw_60m.columns:\n",
    "        df_otpw_60m = df_otpw_60m.withColumn(col, F.col(col).cast(\"timestamp\"))\n",
    "\n",
    "# Cast columns to int\n",
    "cast_cols_to_int = [\"TAXI_OUT\", \"TAXI_IN\", \"DEP_DEL15\",'FLIGHTS']\n",
    "\n",
    "for col_name in cast_cols_to_int:\n",
    "    if col in df_otpw_60m.columns:\n",
    "        df_otpw_60m = df_otpw_60m.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "# Fill null values because null means there was not a delay. In other words, 0 minutes delay\n",
    "# df_otpw_12m = df_otpw_12m.fillna(0, subset=['CARRIER_DELAY','WEATHER_DELAY','NAS_DELAY','SECURITY_DELAY','LATE_AIRCRAFT_DELAY'])\n",
    "\n",
    "# NOTE TO SELF, CIRLCE BACK TO CLEAN HourlyPressureChange. NEED TO DEAL WITH '+' '-'\n",
    "# HourlySkyConditions,REM MAYBE FOR TEXT PROCESSING?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9953f3f5-cb33-4537-8c67-297cd4b51afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"HourlyPrecipitation\",df_otpw_60m.filter(F.col(\"HourlyPrecipitation\").isNull()).count())\n",
    "print(\"HourlySeaLevelPressure\",df_otpw_60m.filter(F.col(\"HourlySeaLevelPressure\").isNull()).count())\n",
    "print(\"HourlyWindGustSpeed\",df_otpw_60m.filter(F.col(\"HourlyWindGustSpeed\").isNull()).count())\n",
    "print(\"HourlyPressureTendency\",df_otpw_60m.filter(F.col(\"HourlyPressureTendency\").isNull()).count())\n",
    "print(\"HourlyDewPointTemperature\",df_otpw_60m.filter(F.col(\"HourlyDewPointTemperature\").isNull()).count())\n",
    "print(\"HourlyDryBulbTemperature\",df_otpw_60m.filter(F.col(\"HourlyDryBulbTemperature\").isNull()).count())\n",
    "print(\"HourlyStationPressure\",df_otpw_60m.filter(F.col(\"HourlyStationPressure\").isNull()).count())\n",
    "print(\"HourlyVisibility\",df_otpw_60m.filter(F.col(\"HourlyVisibility\").isNull()).count())\n",
    "print(\"HourlyWetBulbTemperature\",df_otpw_60m.filter(F.col(\"HourlyWetBulbTemperature\").isNull()).count())\n",
    "print(\"HourlyWindSpeed\",df_otpw_60m.filter(F.col(\"HourlyWindSpeed\").isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459c7909-8be6-47ab-a821-aaf8d1e7fb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write dictionary with list of redundant columns, keys are the columns we will be keeping\n",
    "redundant_columns = {\n",
    "    \"OP_CARRIER_AIRLINE_ID\" : [\"OP_CARRIER\",\"OP_UNIQUE_CARRIER\"], # these are duplicate columns, relationship is 1:1 between ID and carrier\n",
    "    \"ORIGIN_AIRPORT_ID\" : [\"ORIGIN_AIRPORT_SEQ_ID\",\"origin_airport_name\",\"origin_station_name\",\"origin_station_id\",\"origin_region\",\"NAME\",\"ORIGIN_CITY_NAME\"],  # ORIGIN_AIRPORT_SEQ_ID is the same with a marker for point in time\n",
    "    \"ORIGIN_CITY_MARKET_ID\" : [\"\"], # including to group airports that service the same market\n",
    "    \"ORIGIN\" : [\"origin_iata_code\",\"origin_icao\"], # these are duplicate columns, i.e. LGA = LGA\n",
    "    \"ORIGIN_STATE_ABR\" : [\"ORIGIN_STATE_NM\"], # one is just the abbreviation\n",
    "    \"DEST_AIRPORT_ID\" : [\"DEST_AIRPORT_SEQ_ID\",\"dest_airport_name\",\"dest_station_name\",\"dest_station_id\",\"dest_region\",\"DEST_CITY_NAME\"],  # DEST_AIRPORT_SEQ_ID is the same with a marker for point in time\n",
    "    \"DEST_CITY_MARKET_ID\" : [], # including to group airports that service the same market\n",
    "    \"DEST_STATE_ABR\" : [\"DEST_STATE_NM\"], # these are duplicate columns, i.e. LGA = LGA\n",
    "    \"DEP_DELAY\" : [\"CRS_DEP_TIME\",\"DEP_TIME\",\"DEP_TIME_BLK\",\"DEP_DELAY_GROUP\",\n",
    "                   \"TAXI_OUT\",\"WHEELS_OFF\",\"WHEELS_ON\",\"TAXI_IN\",\"CRS_ARR_TIME\",\n",
    "                   \"ARR_TIME\",\"ARR_DELAY\",\"ARR_DELAY_NEW\",\"ARR_DEL15\",\"ARR_DELAY_GROUP\",\"ARR_TIME_BLK\",\"CRS_ELAPSED_TIME\",\"ACTUAL_ELAPSED_TIME\",\n",
    "                   \"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\"LATE_AIRCRAFT_DELAY\"], # dep delay and related columns that are not relevant such as arrival time variables and taxi/wheels information we wouldn't have 2 hours prior\n",
    "    \"DEL_DELAY_NEW\": [], # keeping DEP_DELAY_NEW for potential target variable\n",
    "    \"DEST\" : [\"dest_iata_code\",\"dest_icao\"], # these are duplicate columns, i.e. LGA = LGA\n",
    "    \"DEST_STATE_ABR\" : [\"DEST_STATE_NM\"], # one is just the abbreviation,\n",
    "    \"DATE\": [\"QUARTER\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"FL_DATE\",\"YEAR\"], # keep \"MONTH\"\n",
    "    \"MISCELLANEOUS\": [\"REPORT_TYPE\",\"SOURCE\",\"REM\",\"BackupDirection\",\"BackupDistance\",\"BackupDistanceUnit\",\"BackupElements\",\"BackupElevation\",\"BackupEquipment\",\t\"BackupLatitude\",\"BackupLongitude\",\"BackupName\",\"WindEquipmentChangeDate\",\"_row_desc\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904f8cda-8840-4a77-9ecf-fa3cd24cace5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "cols_to_drop = list(itertools.chain.from_iterable(redundant_columns.values()))\n",
    "df_otpw_60m = df_otpw_60m.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12de435-cf3b-4aa4-9bf9-49cc22b2cca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "# Filter out the missing values in the target variable\n",
    "df_otpw_60m = df_otpw_60m.filter(\n",
    "    (col(\"DEP_DEL15\").isNotNull()) & \n",
    "    (col(\"CANCELLED\").cast(\"int\") != 1)\n",
    ")\n",
    "\n",
    "# Find columns with only 0 or 1 unique values\n",
    "cols_to_drop_60m = [\n",
    "    c for c in df_otpw_60m.columns \n",
    "    if df_otpw_60m.select(col(c)).distinct().count() <= 1\n",
    "]\n",
    "\n",
    "# Drop these columns\n",
    "df_otpw_60m = df_otpw_60m.drop(*cols_to_drop_60m)\n",
    "\n",
    "# Show dropped columns\n",
    "print(\"Dropped columns in 60-month dataset:\", cols_to_drop_60m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569a6542-09b9-4445-8707-a0ce24e26065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m.write.mode(\"overwrite\").parquet(f\"{folder_path}/otpw_60m_preprocess.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf709cb-e8ba-4028-8731-76978f348475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909730d2-4b4a-48c8-aade-e4fb1bad564c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Feature 1: Previous Flight Delay Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c112c2-a95d-4bb9-afec-492a986eece1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "section = \"02\"\n",
    "number = \"01\"\n",
    "folder_path = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "\n",
    "df_otpw_60m = spark.read.parquet(f\"{folder_path}/otpw_60m_preprocess.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2342ec78-c5fa-45b8-84a6-e89b656d884c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b70dad2-6cc9-4e84-b62d-11ad16b2e5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, when, to_timestamp, unix_timestamp\n",
    "\n",
    "#converting sched_depart_date_time_UTC to timestamp\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"sched_depart_date_time_UTC\", to_timestamp(F.col(\"sched_depart_date_time_UTC\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "#creating window for previous flight delay by grouping by tail number\n",
    "window = Window.partitionBy(\"TAIL_NUM\").orderBy(F.col(\"sched_depart_date_time_UTC\"))\n",
    "\n",
    "#getting arrival delay of previous flight as well as previous flight time\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"prev_flight_delay_raw\", lag(F.col(\"DEP_DELAY_NEW\"), 1).over(window))\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"prev_flight_time\", lag(F.col(\"sched_depart_date_time_UTC\"), 1).over(window))\n",
    "\n",
    "#calculate time difference so we only take previous flight delay if the flight is two hours before\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\n",
    "    \"time_diff_seconds\",\n",
    "    unix_timestamp(F.col(\"sched_depart_date_time_UTC\")) - unix_timestamp(F.col(\"prev_flight_time\"))\n",
    "    )\n",
    "\n",
    "#filtering out flights that are not two hours before\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\n",
    "    \"prev_flight_delay\",\n",
    "    when(F.col(\"time_diff_seconds\") >= 7200, F.col(\"prev_flight_delay_raw\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\n",
    "    \"prev_flight_delay_ind\",\n",
    "    when(F.col(\"prev_flight_delay\") > 0, 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "df_otpw_60m = df_otpw_60m.drop(\"prev_flight_delay_raw\", \"prev_flight_delay\", \"prev_flight_time\", \"time_diff_seconds\")\n",
    "\n",
    "df_otpw_60m.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a075a098-065d-46c9-8e64-a3789b6a965a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#verifying prev_flight_delay results\n",
    "df_otpw_60m.filter(df_otpw_60m.TAIL_NUM == \"N102UW\").select(\"sched_depart_date_time_UTC\", \"DEP_DELAY_NEW\", \"prev_flight_delay_ind\").orderBy(\"sched_depart_date_time_UTC\").display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3006f3-5032-45aa-bf1e-77bf00398af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA on Feature 1\n",
    "# Plot distribution of prev_flight_delay_ind\n",
    "prev_delay_count = df_otpw_60m.groupBy(\"prev_flight_delay_ind\").count().toPandas()\n",
    "\n",
    "# Plot\n",
    "sns.barplot(data=prev_delay_count, x=\"prev_flight_delay_ind\", y=\"count\", palette=\"Set2\")\n",
    "plt.title(\"Distribution of Previous Flight Delay Indicator (5 Years)\")\n",
    "plt.xlabel(\"Previous Flight Delay Indicator\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks([0, 1], [\"No Previous Delay\", \"Previous Delay\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729af3c5-e50a-4574-8892-a8f5a2c736ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature 2: Holiday Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19adb5e-2226-49a2-80ef-133bfa56aa1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col, lit, when\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Create a Python set of buffered holiday dates (±2 days)\n",
    "us_holidays = {\n",
    "    2015: [\"2015-01-01\", \"2015-01-19\", \"2015-02-16\", \"2015-05-25\", \"2015-07-04\",\n",
    "           \"2015-09-07\", \"2015-10-12\", \"2015-11-11\", \"2015-11-26\", \"2015-12-25\"],\n",
    "    2016: [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "           \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-25\"],\n",
    "    2017: [\"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "           \"2017-09-04\", \"2017-10-09\", \"2017-11-11\", \"2017-11-23\", \"2017-12-25\"],\n",
    "    2018: [\"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "           \"2018-09-03\", \"2018-10-08\", \"2018-11-11\", \"2018-11-22\", \"2018-12-25\"],\n",
    "    2019: [\"2019-01-01\", \"2019-01-21\", \"2019-02-18\", \"2019-05-27\", \"2019-07-04\",\n",
    "           \"2019-09-02\", \"2019-10-14\", \"2019-11-11\", \"2019-11-28\", \"2019-12-25\"]\n",
    "}\n",
    "\n",
    "# Step 2: Generate buffered holiday dates as strings\n",
    "holiday_buffer_dates = set()\n",
    "for year, holidays in us_holidays.items():\n",
    "    for h in holidays:\n",
    "        h_date = datetime.strptime(h, \"%Y-%m-%d\")\n",
    "        for offset in range(-2, 3):  # from -2 to +2 days\n",
    "            date_str = (h_date + timedelta(days=offset)).strftime(\"%Y-%m-%d\")\n",
    "            holiday_buffer_dates.add(date_str)\n",
    "\n",
    "# Step 3: Convert sched_depart_date_time_UTC to date (if not already)\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\n",
    "    \"sched_depart_date\", \n",
    "    to_date(col(\"sched_depart_date_time_UTC\"))\n",
    ")\n",
    "\n",
    "# Step 4: Create is_holiday column by checking membership in buffered date list\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\n",
    "    \"is_holiday\",\n",
    "    when(col(\"sched_depart_date\").cast(\"string\").isin(list(holiday_buffer_dates)), lit(1)).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "# Optional: Drop helper column if not needed\n",
    "df_otpw_60m = df_otpw_60m.drop(\"sched_depart_date\")\n",
    "df_otpw_60m.select(\"sched_depart_date_time_UTC\", \"is_holiday\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd41ca1-c644-4e42-a0d3-bb4e8a4b26a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verifying is_holiday results\n",
    "df_otpw_60m.select(\"sched_depart_date_time_UTC\", \"DEP_DELAY_NEW\", \"is_holiday\") \\\n",
    "    .orderBy(\"sched_depart_date_time_UTC\") \\\n",
    "    .display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f170ad-02f4-4a34-b9b3-73bf8d7aacc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA on Feature 2\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "# Holiday vs. Non-Holiday Flight Counts\n",
    "df_otpw_60m.groupBy(\"is_holiday\").count().orderBy(\"is_holiday\").display()\n",
    "\n",
    "# Average Delay times by Holiday Indicator\n",
    "df_otpw_60m.groupBy(\"is_holiday\") \\\n",
    "    .agg(F.avg(\"DEP_DELAY_NEW\").alias(\"avg_dep_delay\"),\n",
    "         F.stddev(\"DEP_DELAY_NEW\").alias(\"stddev_dep_delay\")) \\\n",
    "    .orderBy(\"is_holiday\") \\\n",
    "    .display()\n",
    "\n",
    "# Proportion of Delayed Flights Near Holidays\n",
    "df_otpw_60m.groupBy(\"is_holiday\") \\\n",
    "    .agg(F.avg(\"DEP_DEL15\").alias(\"prop_delayed_15min\")) \\\n",
    "    .orderBy(\"is_holiday\") \\\n",
    "    .display()\n",
    "\n",
    "# Monthly Distribution of Holiday Flights\n",
    "df_otpw_60m.withColumn(\"month\", month(\"sched_depart_date_time_UTC\")) \\\n",
    "    .groupBy(\"month\", \"is_holiday\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"month\", \"is_holiday\") \\\n",
    "    .display()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2a7595-642a-4d45-a257-f2a1a8bfda15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Holiday vs Delay Rate (DEP_DEL15)\n",
    "df_otpw_60m.groupBy(\"is_holiday\").agg(\n",
    "    F.count(\"*\").alias(\"total_flights\"),\n",
    "    F.sum(\"DEP_DEL15\").alias(\"num_delayed\"),\n",
    "    (F.sum(\"DEP_DEL15\") / F.count(\"*\")).alias(\"delay_rate\")\n",
    ").show()\n",
    "\n",
    "\n",
    "# Plot Delay Rates by Holiday Indicator\n",
    "# Convert to pandas for visualization\n",
    "holiday_delay_pd = df_otpw_60m.groupBy(\"is_holiday\").agg(\n",
    "    F.count(\"*\").alias(\"total_flights\"),\n",
    "    F.sum(\"DEP_DEL15\").alias(\"num_delayed\"),\n",
    "    (F.sum(\"DEP_DEL15\") / F.count(\"*\")).alias(\"delay_rate\")\n",
    ").toPandas()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=holiday_delay_pd, x=\"is_holiday\", y=\"delay_rate\", palette=\"Set2\")\n",
    "plt.xticks([0, 1], [\"Non-Holiday\", \"Holiday (±2 Days)\"])\n",
    "plt.ylabel(\"Delay Rate\")\n",
    "plt.title(\"Flight Delay Rate by Holiday Indicator (5 Year)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7990bf2b-d7c7-4db8-a35f-c61e9ec22acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by holiday indicator and calculate average DEP_DELAY_NEW (delay time difference)\n",
    "holiday_delay_diff = df_otpw_60m.groupBy(\"is_holiday\").agg(\n",
    "    F.avg(\"DEP_DELAY_NEW\").alias(\"avg_delay_time_diff\")\n",
    ").toPandas()\n",
    "\n",
    "# Map for better labels\n",
    "holiday_delay_diff[\"is_holiday\"] = holiday_delay_diff[\"is_holiday\"].map({0: \"Non-Holiday\", 1: \"Holiday\"})\n",
    "\n",
    "# Plot average delay time difference (DEP_DELAY_NEW)\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.barplot(data=holiday_delay_diff, x=\"is_holiday\", y=\"avg_delay_time_diff\", palette=\"Set2\")\n",
    "\n",
    "# Add annotations for the average delay times\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{height:.2f}',  # Format as two decimal places\n",
    "                (p.get_x() + p.get_width() / 2., height), \n",
    "                ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "plt.ylabel(\"Average Delay Time In Minutes\")\n",
    "plt.xlabel(\"Holiday Indicator\")\n",
    "plt.title(\"Average Delay Time by Holiday Indicator (5 Year)\")\n",
    "plt.xticks([0, 1], [\"Non-Holiday\", \"Holiday (±2 Days)\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57342d58-d351-4e0c-a738-8b6c1b5b6310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by holiday indicator and calculate average DEP_DELAY_NEW (delay time difference)\n",
    "holiday_delay_diff = df_otpw_60m.groupBy(\"is_holiday\").agg(\n",
    "    F.avg(\"DEP_DELAY_NEW\").alias(\"avg_delay_time_diff\")\n",
    ").toPandas()\n",
    "\n",
    "# Map for better labels\n",
    "holiday_delay_diff[\"is_holiday\"] = holiday_delay_diff[\"is_holiday\"].map({0: \"Non-Holiday\", 1: \"Holiday\"})\n",
    "\n",
    "# Group by holiday indicator and calculate delay rates\n",
    "holiday_delay_pd = df_otpw_60m.groupBy(\"is_holiday\").agg(\n",
    "    F.count(\"*\").alias(\"total_flights\"),\n",
    "    F.sum(\"DEP_DEL15\").alias(\"num_delayed\"),\n",
    "    (F.sum(\"DEP_DEL15\") / F.count(\"*\")).alias(\"delay_rate\")\n",
    ").toPandas()\n",
    "\n",
    "# Set up the figure for side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Average Delay Time by Holiday Indicator\n",
    "ax1 = axes[0]\n",
    "sns.barplot(data=holiday_delay_diff, x=\"is_holiday\", y=\"avg_delay_time_diff\", palette=\"Set2\", ax=ax1)\n",
    "\n",
    "# Add annotations for the average delay times on the first plot\n",
    "for p in ax1.patches:\n",
    "    height = p.get_height()\n",
    "    ax1.annotate(f'{height:.2f}', \n",
    "                 (p.get_x() + p.get_width() / 2., height), \n",
    "                 ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "ax1.set_ylabel(\"Average Delay Time In Minutes\")\n",
    "ax1.set_xlabel(\"Holiday Indicator\")\n",
    "ax1.set_title(\"Average Delay Time by Holiday Indicator (5 Year)\")\n",
    "ax1.set_xticklabels([\"Non-Holiday\", \"Holiday (±2 Days)\"])\n",
    "\n",
    "# Plot 2: Delay Rate by Holiday Indicator\n",
    "ax2 = axes[1]\n",
    "sns.barplot(data=holiday_delay_pd, x=\"is_holiday\", y=\"delay_rate\", palette=\"Set2\", ax=ax2)\n",
    "\n",
    "# Add annotations for the delay rates on the second plot\n",
    "for p in ax2.patches:\n",
    "    height = p.get_height()\n",
    "    ax2.annotate(f'{height * 100:.2f}%', \n",
    "                 (p.get_x() + p.get_width() / 2., height), \n",
    "                 ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel(\"Delay Rate\")\n",
    "ax2.set_xlabel(\"Holiday Indicator\")\n",
    "ax2.set_title(\"Flight Delay Rate by Holiday Indicator (5 Year)\")\n",
    "ax2.set_xticklabels([\"Non-Holiday\", \"Holiday (±2 Days)\"])\n",
    "\n",
    "# Tighten layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e53abd-f4ef-4a17-8512-225bd7d10d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to pandas\n",
    "holiday_counts = df_otpw_60m.groupBy(\"is_holiday\").count().toPandas()\n",
    "\n",
    "# Plot\n",
    "sns.barplot(data=holiday_counts, x=\"is_holiday\", y=\"count\", palette=\"Set2\")\n",
    "plt.title(\"Flight Count: Holiday vs. Non-Holiday (5 Years)\")\n",
    "plt.xlabel(\"Is Holiday (±2 days)\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks([0, 1], [\"Non-Holiday\", \"Holiday\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d56ae61-8273-4309-8c7e-798de4a7c96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "# Flight Counts by Holiday Indicator and Year\n",
    "# Add year column to Spark DataFrame\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"year\", year(\"sched_depart_date_time_UTC\"))\n",
    "\n",
    "# Group by year and is_holiday\n",
    "holiday_counts_by_year = df_otpw_60m.groupBy(\"year\", \"is_holiday\").count().orderBy(\"year\", \"is_holiday\").toPandas()\n",
    "\n",
    "# Map for better labels\n",
    "holiday_counts_by_year[\"is_holiday\"] = holiday_counts_by_year[\"is_holiday\"].map({0: \"Non-Holiday\", 1: \"Holiday\"})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=holiday_counts_by_year, x=\"year\", y=\"count\", hue=\"is_holiday\", palette=\"Set2\")\n",
    "plt.title(\"Flight Counts by Year: Holiday vs. Non-Holiday (5 Years)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.legend(title=\"Is Holiday\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5580d5ed-0de9-4c6b-b7a8-13656c05b259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Add year column to Spark DataFrame\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\"year\", year(\"sched_depart_date_time_UTC\"))\n",
    "\n",
    "# Step 2: Group by year and is_holiday to get counts\n",
    "holiday_counts_by_year = df_otpw_60m.groupBy(\"year\", \"is_holiday\").count().toPandas()\n",
    "\n",
    "# Step 3: Compute total flights per year\n",
    "total_per_year = holiday_counts_by_year.groupby(\"year\")[\"count\"].transform(\"sum\")\n",
    "holiday_counts_by_year[\"percentage\"] = holiday_counts_by_year[\"count\"] / total_per_year\n",
    "\n",
    "# Step 4: Map is_holiday values to labels for better readability\n",
    "holiday_counts_by_year[\"is_holiday\"] = holiday_counts_by_year[\"is_holiday\"].map({0: \"Non-Holiday\", 1: \"Holiday\"})\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    data=holiday_counts_by_year, \n",
    "    x=\"year\", y=\"percentage\", hue=\"is_holiday\", palette=\"Set2\"\n",
    ")\n",
    "\n",
    "# Add annotations to show percentages on the bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{height * 100:.1f}%', \n",
    "                (p.get_x() + p.get_width() / 2., height), \n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Final touches\n",
    "plt.title(\"Percentage of Flights by Year: Holiday vs. Non-Holiday (2015–2019)\")\n",
    "plt.ylabel(\"Percentage of Flights\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Is Holiday\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c044db67-5aef-411c-8a0c-be9f8fe94737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "# Monthly Flight Volume by Holiday Indicator\n",
    "from pyspark.sql.functions import month\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Monthly Flight Volume by Holiday Indicator\n",
    "monthly = df_otpw_60m.withColumn(\"month\", month(\"sched_depart_date_time_UTC\")) \\\n",
    "    .groupBy(\"month\", \"is_holiday\").count() \\\n",
    "    .orderBy(\"month\", \"is_holiday\") \\\n",
    "    .toPandas()\n",
    "\n",
    "# Convert is_holiday to string for better legend labels\n",
    "monthly[\"is_holiday\"] = monthly[\"is_holiday\"].map({0: \"Non-Holiday\", 1: \"Holiday\"})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=monthly, x=\"month\", y=\"count\", hue=\"is_holiday\", palette=\"Set2\")\n",
    "plt.title(\"Monthly Flight Counts by Holiday Indicator (5 Years)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Flight Count\")\n",
    "plt.legend(title=\"Is Holiday\")  # Let Seaborn/Matplotlib auto-handle colors\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526ee3b0-2e44-48b2-b75e-8c30397eb857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m = df_otpw_60m.drop(\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a47a90-83c0-49a2-8690-364961d4dcee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature 3: Weekend Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ec023d-f927-4fd0-a37b-f3cf2f45db60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek, lit, when, hour\n",
    "\n",
    "df_otpw_60m = df_otpw_60m.withColumn(\n",
    "    \"is_weekend\",\n",
    "    when(dayofweek(col(\"sched_depart_date_time_UTC\")) == 1, lit(1))  # Sunday\n",
    "    .when(dayofweek(col(\"sched_depart_date_time_UTC\")) == 7, lit(1))  # Saturday\n",
    "    .otherwise(lit(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df91358c-7d75-4287-a956-0c18f9390cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by both 'is_weekend' and 'DEP_DEL15', then count the occurrences\n",
    "delay_counts = df_otpw_60m.groupBy(\"is_weekend\", \"DEP_DEL15\") \\\n",
    "    .agg(F.count(\"*\").alias(\"flight_count\")) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Plotting\n",
    "sns.barplot(data=delay_counts, x=\"is_weekend\", y=\"flight_count\", hue=\"DEP_DEL15\")\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xticks([0, 1], [\"Weekday\", \"Weekend\"])\n",
    "plt.ylabel(\"Count of Flights\")\n",
    "plt.title(\"Delayed vs Non-Delayed Flights: Weekend vs Weekday (5 Year)\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles=handles, labels=[\"Not Delayed\", \"Delayed\"], title=\"Delay Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d4e2a0-22d8-4330-b2c2-22a13d11040e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pivot data to get counts for pie chart\n",
    "pivot_df = delay_counts.pivot(index='DEP_DEL15', columns='is_weekend', values='flight_count').fillna(0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Weekday pie\n",
    "axes[0].pie(pivot_df[0], labels=['Not Delayed', 'Delayed'], autopct='%1.1f%%', colors=sns.color_palette(\"pastel\"))\n",
    "axes[0].set_title('Weekday Flights')\n",
    "\n",
    "# Weekend pie\n",
    "axes[1].pie(pivot_df[1], labels=['Not Delayed', 'Delayed'], autopct='%1.1f%%', colors=sns.color_palette(\"pastel\"))\n",
    "axes[1].set_title('Weekend Flights')\n",
    "\n",
    "plt.suptitle('Delay vs Non-Delay Flight %: Weekday vs Weekend (5 Year)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6722573-a41c-47f9-bf11-402b80402391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by 'is_weekend' and 'DEP_DEL15', calculate the average delay for each group\n",
    "delay_avg = df_otpw_60m.groupBy(\"is_weekend\") \\\n",
    "    .agg(F.avg(\"DEP_DELAY_NEW\").alias(\"avg_delay\")) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Plotting the average delay\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.barplot(data=delay_avg, x=\"is_weekend\", y=\"avg_delay\")\n",
    "\n",
    "# Adding the value annotations for each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{height:.2f}',  # Format as two decimal places\n",
    "                (p.get_x() + p.get_width() / 2., height), \n",
    "                ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xticks([0, 1], [\"Weekday\", \"Weekend\"])\n",
    "plt.ylabel(\"Average Delay (Minutes)\")\n",
    "plt.title(\"Average Flight Delay: Weekend vs Weekday (5 Year)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a86f95-cada-408d-9fbb-8fb7b45eab54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature 4: Airports Network (Page Rank and Degree Centrality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b471cdc3-e78e-406f-a459-86c735952002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cda8df1-bc78-4d92-b5e2-26fd93a3b192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "# find the edges of the graph (airport routes) and use the count as the weight\n",
    "edges = df_otpw_60m.groupBy('ORIGIN', 'DEST') \\\n",
    "    .agg(F.count('*').alias('weight'))\n",
    "\n",
    "edges.cache()\n",
    "\n",
    "display(edges)\n",
    "\n",
    "# Compute total weight per ORIGIN (src)\n",
    "total_flights = edges.groupBy('ORIGIN').agg(F.sum('weight').alias('total_weight'))\n",
    "\n",
    "# Join and calculate normalized prob\n",
    "edges_with_prob = edges.join(total_flights, on='ORIGIN') \\\n",
    "    .withColumn('prob', F.col('weight') / F.col('total_weight'))\n",
    "\n",
    "edges_with_prob.cache()\n",
    "\n",
    "# get the vertices of the graph (airports)\n",
    "vertices = edges.select('ORIGIN').union(edges.select('DEST')) \\\n",
    "    .distinct().withColumnRenamed('ORIGIN', 'id')\n",
    "\n",
    "vertices.cache()\n",
    "\n",
    "# create the edges for the graph\n",
    "edges_for_graph = edges_with_prob.selectExpr('ORIGIN as src', 'DEST as dst', 'prob')\n",
    "\n",
    "edges_for_graph.cache()\n",
    "\n",
    "# create the graph\n",
    "g = GraphFrame(vertices, edges_for_graph)\n",
    "\n",
    "# run the pagerank of the graph\n",
    "results = g.pageRank(resetProbability=0.15, maxIter=20)\n",
    "results.vertices.select('id', 'pagerank').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cc1c87-c3d5-4960-aa5b-65088d136235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the degree centrality from GraphFrame\n",
    "total_degree_df = g.degrees.toPandas()\n",
    "\n",
    "# Sort the airports by degree centrality\n",
    "total_degree_df = total_degree_df.sort_values(by='degree', ascending=False)\n",
    "\n",
    "# Calculate normalized degree centrality\n",
    "N = total_degree_df.shape[0]  # Total number of airports\n",
    "total_degree_df['normalized_degree_centrality'] = total_degree_df['degree'] / (N - 1)\n",
    "\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(total_degree_df['id'], total_degree_df['degree'], color='skyblue', edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Airport', fontsize=12)\n",
    "plt.ylabel('Degree (Number of Connections)', fontsize=12)\n",
    "plt.title('Degree Centrality of Airports (5 Years)', fontsize=16)\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "# Only display every 5-th airport name on the x-axis for readability\n",
    "n = 5  \n",
    "plt.xticks(total_degree_df['id'][::n], rotation=90)  # Show every n-th airport name\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e67d71df-a75f-4b68-a3d4-46682ad6035b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(total_degree_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c138e924-a594-4099-9afb-e5b160e901cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# In-degree: Count the number of incoming edges (flights arriving at the airport)\n",
    "in_degree_df = g.inDegrees.toPandas()\n",
    "\n",
    "# Out-degree: Count the number of outgoing edges (flights departing from the airport)\n",
    "out_degree_df = g.outDegrees.toPandas()\n",
    "\n",
    "# Merge in-degree and out-degree data\n",
    "degree_df = pd.merge(in_degree_df, out_degree_df, on='id', how='outer')\n",
    "degree_df = degree_df.rename(columns={'inDegree': 'in_degree', 'outDegree': 'out_degree'})\n",
    "\n",
    "# Sort the airports by in-degree centrality\n",
    "degree_df = degree_df.sort_values(by='out_degree', ascending=False)\n",
    "\n",
    "# Create subplots for In-degree and Out-degree\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 15), sharex=True)\n",
    "\n",
    "# Plotting the In-degree Centrality\n",
    "ax1.bar(degree_df['id'], degree_df['in_degree'], color='skyblue', edgecolor='black', alpha=0.6)\n",
    "ax1.set_title('In-degree Centrality of Airports (5 Years)', fontsize=16)\n",
    "ax1.set_ylabel('In-degree (Number of Incoming Flights)', fontsize=12)\n",
    "\n",
    "# Plotting the Out-degree Centrality\n",
    "ax2.bar(degree_df['id'], degree_df['out_degree'], color='green', edgecolor='black', alpha=0.6)\n",
    "ax2.set_title('Out-degree Centrality of Airports (5 Years)', fontsize=16)\n",
    "ax2.set_xlabel('Airport', fontsize=12)\n",
    "ax2.set_ylabel('Out-degree (Number of Departing Flights)', fontsize=12)\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "n = 8  # Adjust this value to show every n-th airport\n",
    "ax2.set_xticks(degree_df['id'][::n])  # Show every n-th airport name\n",
    "ax2.set_xticklabels(degree_df['id'][::n], rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0e866d-69e6-4b3a-a5a2-d56bc55ee29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the degree centralities\n",
    "N = degree_df.shape[0]  # Total number of airports\n",
    "degree_df['normalized_in_degree'] = degree_df['in_degree'] / (N - 1)\n",
    "degree_df['normalized_out_degree'] = degree_df['out_degree'] / (N - 1)\n",
    "display(degree_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4697e559-d681-4752-be6d-7eb60ee1bae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, convert degree_df (Pandas) to a Spark DataFrame\n",
    "degree_sdf = spark.createDataFrame(degree_df)\n",
    "\n",
    "# Rename columns to avoid ambiguity during join\n",
    "origin_degree = degree_sdf.select(\n",
    "    F.col(\"id\").alias(\"ORIGIN\"),\n",
    "    F.col(\"normalized_out_degree\").alias(\"origin_normalized_out_degree\")\n",
    ")\n",
    "\n",
    "dest_degree = degree_sdf.select(\n",
    "    F.col(\"id\").alias(\"DEST\"),\n",
    "    F.col(\"normalized_in_degree\").alias(\"dest_normalized_in_degree\")\n",
    ")\n",
    "\n",
    "# Join on ORIGIN\n",
    "df_otpw_60m = df_otpw_60m.join(origin_degree, on=\"ORIGIN\", how=\"left\")\n",
    "\n",
    "# Then join on DEST\n",
    "df_otpw_60m = df_otpw_60m.join(dest_degree, on=\"DEST\", how=\"left\")\n",
    "\n",
    "# Review the results\n",
    "df_otpw_60m.select(\n",
    "    \"ORIGIN\", \"DEST\",\n",
    "    \"origin_normalized_out_degree\",\n",
    "    \"dest_normalized_in_degree\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619d0df3-dac0-4b7d-a43a-a73be2ccc162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Add a PageRank score to the original 5 Years DataFrame\n",
    "\n",
    "pagerank_df = results.vertices\n",
    "\n",
    "# join the pagerank table using broadcast for both origin and destination airport\n",
    "df_otpw_60m = df_otpw_60m \\\n",
    "    .join(broadcast(pagerank_df), df_otpw_60m.ORIGIN == pagerank_df.id, \"left\") \\\n",
    "    .withColumnRenamed(\"pagerank\", \"origin_pagerank\") \\\n",
    "    .drop(\"id\") \\\n",
    "    .join(broadcast(pagerank_df), df_otpw_60m.DEST == pagerank_df.id, \"left\") \\\n",
    "    .withColumnRenamed(\"pagerank\", \"dest_pagerank\") \\\n",
    "    .drop(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848672b7-9ae2-4b97-b551-c66288fbac71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Check Point Data Set After Pre-Processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107fbdff-b4cd-4df1-9189-94df50d86c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checkpoint the data after data pre-processing\n",
    "df_otpw_60m.write.mode(\"overwrite\").parquet(f\"{folder_path}/otpw_60m_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff537c5e-9563-4933-ab61-eb982244b21b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EDA Post Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725b1b83-508b-4957-bf48-19603d0926a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "section = \"02\"\n",
    "number = \"01\"\n",
    "folder_path = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "\n",
    "df_otpw_60m = spark.read.parquet(f\"{folder_path}/otpw_60m_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c8d7b3-5a7e-4908-8fa0-0cb1792bd05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbce2d31-bc46-4c29-bba4-e427480ba4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m.limit(10).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a4019d-2344-42b2-bc16-087be53862df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the 60 month data to plot the Geo plot\n",
    "# create a geo plot for all airports and the selected US weather stations (closest to each airport)\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# create a table for destination airport and latitude and longitude\n",
    "dest = df_otpw_60m.dropDuplicates(['DEST']).select('DEST', 'dest_airport_lat', 'dest_airport_lon')\n",
    "\n",
    "display(dest)\n",
    "dests = dest.toPandas()\n",
    "\n",
    "# create a table for weather station and latitude and longitude\n",
    "station = df_otpw_60m.dropDuplicates(['STATION']).select('STATION', 'LATITUDE', 'LONGITUDE')\n",
    "\n",
    "display(station)\n",
    "\n",
    "# Set up plotting tables\n",
    "stations = station.toPandas()\n",
    "# stations = stations_us.toPandas()\n",
    "\n",
    "# Set up map\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "         lat = dests['dest_airport_lat'],\n",
    "         lon = dests['dest_airport_lon'],\n",
    "         text = dests['DEST'],\n",
    "         name = 'Airport',\n",
    "         marker_symbol = 'circle-dot',\n",
    "         marker_color = 'red',))\n",
    "fig.add_trace(go.Scattergeo(\n",
    "         lat = stations['LATITUDE'],\n",
    "         lon = stations['LONGITUDE'],\n",
    "         marker_color = 'green',\n",
    "         name = 'Weather Stations',\n",
    "         marker_symbol = 'x-dot',\n",
    "         marker_size = 3,\n",
    "         opacity = 0.5))\n",
    "fig.update_layout(\n",
    "         title = 'US Airports & Weather Stations (5 Years)',\n",
    "         geo_scope='usa',\n",
    "         autosize=False,\n",
    "         width=1000,\n",
    "         height=700,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47eae7c6-53b3-46af-bf2b-ea12e8ecfac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "from pyspark.sql.functions import count\n",
    "import plotly.graph_objects as go\n",
    "#Geo plot of Origin airports with avg delay on color (60 month)\n",
    "origin_delays = df_otpw_60m.groupBy(\"ORIGIN\", \"ORIGIN_AIRPORT_LAT\", \"ORIGIN_AIRPORT_LON\").agg(avg(\"DEP_DELAY\").alias(\"AVG_DEP_DELAY\"))\n",
    "\n",
    "#create table with only destination airport, latitude and longitude, and average departure delay\n",
    "origin_delays.orderBy(col(\"AVG_DEP_DELAY\").desc()).limit(10).display(10)\n",
    "origin_delays_pd = origin_delays.toPandas()\n",
    "\n",
    "# Set up map\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "         lat = origin_delays_pd['ORIGIN_AIRPORT_LAT'],\n",
    "         lon = origin_delays_pd['ORIGIN_AIRPORT_LON'],\n",
    "         text=origin_delays_pd['ORIGIN'] + \"<br>Avg Delay: \" + origin_delays_pd['AVG_DEP_DELAY'].round(3).astype(str),\n",
    "         marker=dict(\n",
    "            size= 10, \n",
    "            color=origin_delays_pd['AVG_DEP_DELAY'],\n",
    "            colorscale=\"RdBu\",\n",
    "            cmin=-max(abs(origin_delays_pd['AVG_DEP_DELAY'])),\n",
    "            cmax=max(origin_delays_pd['AVG_DEP_DELAY']),\n",
    "            colorbar=dict(title=\"Avg Departure Delay (minutes)\")\n",
    "        )))\n",
    "fig.update_layout(\n",
    "         title = 'Average Departure Delay for Each Origin Airport (5 Years)',\n",
    "         geo_scope='usa',\n",
    "         autosize=False,\n",
    "         width=1000,\n",
    "         height=700,)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9973ec-93b0-4b29-9a08-ca4d8daa828f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, count, sum as spark_sum\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# create a geo plot to show the percentage of delayed flights for each airport\n",
    "\n",
    "# Aggregate: count total flights & delayed flights per airport\n",
    "origin_delay_rate = df_otpw_60m.groupBy(\"ORIGIN\", \"ORIGIN_AIRPORT_LAT\", \"ORIGIN_AIRPORT_LON\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"TOTAL_FLIGHTS\"),\n",
    "        spark_sum(\"DEP_DEL15\").alias(\"DELAYED_FLIGHTS\")\n",
    "    ) \\\n",
    "    .withColumn(\"PCT_DELAY\", (col(\"DELAYED_FLIGHTS\") / col(\"TOTAL_FLIGHTS\")) * 100)\n",
    "\n",
    "# View top 10 worst airports\n",
    "origin_delay_rate.orderBy(col(\"PCT_DELAY\").desc()).limit(10).display()\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "origin_delay_rate_pd = origin_delay_rate.toPandas()\n",
    "\n",
    "# Plotting\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "         lat = origin_delay_rate_pd['ORIGIN_AIRPORT_LAT'],\n",
    "         lon = origin_delay_rate_pd['ORIGIN_AIRPORT_LON'],\n",
    "         text=origin_delay_rate_pd['ORIGIN'] + \"<br>% Delay: \" + origin_delay_rate_pd['PCT_DELAY'].round(2).astype(str) + \"%\",\n",
    "         marker=dict(\n",
    "            size= 10, \n",
    "            color=origin_delay_rate_pd['PCT_DELAY'],\n",
    "            colorscale=\"Reds\",\n",
    "            cmin=0,\n",
    "            cmax=origin_delay_rate_pd['PCT_DELAY'].max(),\n",
    "            colorbar=dict(title=\"% of Delayed Flights\")\n",
    "        )))\n",
    "fig.update_layout(\n",
    "         title = 'Percentage of Delayed Departures for Each Origin Airport (5 Years)',\n",
    "         geo_scope='usa',\n",
    "         autosize=False,\n",
    "         width=1000,\n",
    "         height=700)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1963b24-bf35-4f78-a5e2-96c784b2301b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA on flight delayed time between origin and destination - 60 month data\n",
    "\n",
    "from pyspark.sql.functions import col, avg, expr\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute average departure delay for each origin-destination pair\n",
    "avg_delay_df = df_otpw_60m.groupBy(\"ORIGIN\", \"DEST\").agg(avg(\"DEP_DELAY\").alias(\"AVG_DEP_DELAY\"))\n",
    "\n",
    "# Sort by delay\n",
    "avg_delay_df = avg_delay_df.orderBy(col(\"AVG_DEP_DELAY\").desc())\n",
    "avg_delay_df.display(10)\n",
    "\n",
    "# plot the top 50 routes with the highest delays\n",
    "avg_delay_df50 = avg_delay_df.withColumn(\"ORIGIN-DEST\", expr(\"ORIGIN || '-' || DEST\")).limit(50)\n",
    "avg_delay_df_pd = avg_delay_df50.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=avg_delay_df_pd, x=\"ORIGIN-DEST\", y=\"AVG_DEP_DELAY\", color = sns.color_palette(\"Blues\")[2])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Origin - Destination\")\n",
    "plt.ylabel(\"Average Departure Delay (minutes)\")\n",
    "plt.title(\"Average Departure Delay Between Origin and Destination (5 Years)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f390e0cb-48b8-4f35-bb2b-09348a964776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA on Airport Degree Features\n",
    "\n",
    "plot_df = df_otpw_60m.select(\n",
    "    \"origin_normalized_out_degree\", \"dest_normalized_in_degree\", \"DEP_DEL15\"\n",
    ").dropna().toPandas()\n",
    "\n",
    "# Boxplots of Degree Centrality vs. DEP_DEL15\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=plot_df, x=\"DEP_DEL15\", y=\"origin_normalized_out_degree\", palette=\"Blues\")\n",
    "plt.title(\"Origin Out-Degree vs Delay (5 Years)\")\n",
    "plt.xlabel(\"DEP_DEL15 (0 = No Delay, 1 = Delayed)\")\n",
    "plt.ylabel(\"Normalized Origin Out-Degree\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=plot_df, x=\"DEP_DEL15\", y=\"dest_normalized_in_degree\", palette=\"Reds\")\n",
    "plt.title(\"Destination In-Degree vs Delay (5 Years)\")\n",
    "plt.xlabel(\"DEP_DEL15 (0 = No Delay, 1 = Delayed)\")\n",
    "plt.ylabel(\"Normalized Destination In-Degree\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2552ac16-7019-40ec-a897-e562acd7dd4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA on PageRank Results\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# create a plot for pagerank vs delay rate\n",
    "df_group = df_otpw_60m.groupBy('ORIGIN').agg(\n",
    "    F.avg('origin_pagerank').alias('avg_origin_pagerank'),\n",
    "    F.sum('DEP_DEL15').alias('total_delays'),\n",
    "    F.count('*').alias('total_flights')\n",
    ").withColumn(\n",
    "    'delay_rate', F.col('total_delays') / F.col('total_flights')\n",
    ")\n",
    "\n",
    "# convert the table to Pandas\n",
    "df_pd = df_group.toPandas()\n",
    "\n",
    "# create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(\n",
    "    data=df_pd, \n",
    "    x='avg_origin_pagerank', \n",
    "    y='delay_rate', \n",
    "    size='total_flights', \n",
    "    hue='delay_rate', \n",
    "    palette='Reds', \n",
    "    alpha=0.9\n",
    ")\n",
    "plt.title('Pagerank vs Delay Rate by Airport')\n",
    "plt.xlabel('Pagerank')\n",
    "plt.ylabel('Delay Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77500ae7-6ee5-422d-ae48-d4466b1a140f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA - PageRank and Origin Airports\n",
    "df_group = df_otpw_60m.groupBy('ORIGIN').agg(\n",
    "    F.avg('origin_pagerank').alias('avg_origin_pagerank')\n",
    ").orderBy(F.desc('avg_origin_pagerank'))\n",
    "\n",
    "df_pd = df_group.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax2 = sns.barplot(data=df_pd, x='ORIGIN', y='avg_origin_pagerank',palette='Blues_r')\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "n = 8  # Adjust this value to show every n-th airport\n",
    "ax2.set_xticks(range(0, len(df_pd), n))  # Show every n-th airport name\n",
    "ax2.set_xticklabels(df_pd['ORIGIN'][::n], rotation=90)\n",
    "\n",
    "plt.title('Average Pagerank by Origin Airport (5 Years)')\n",
    "plt.ylabel('Average Pagerank')\n",
    "plt.xlabel('Origin Airport')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6e60ca-1a40-4267-99df-efddfd2b301f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, expr\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Box Plot: Previous Flight Delay Indicator vs Current Departure Delay - 60 months\n",
    "df_sample = df_otpw_60m.select(\"prev_flight_delay_ind\", \"DEP_DELAY_NEW\",\"DEP_DEL15\").toPandas()\n",
    "\n",
    "# Bar plot: % Delayed by previous delay indicator\n",
    "delay_rate = df_sample.groupby(\"prev_flight_delay_ind\")[\"DEP_DEL15\"].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=delay_rate, x=\"prev_flight_delay_ind\", y=\"DEP_DEL15\", palette=\"Set2\")\n",
    "plt.title(\"Proportion of Delays by Previous Flight Delay Indicator (5 Years)\")\n",
    "plt.xlabel(\"Previous Flight Delayed (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Proportion of Delayed Flights (DEP_DEL15)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a540b11a-cf73-464b-bfa9-b1948c70c220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count Plot of Delay Status by Previous Delay Indicator (60m)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_sample, x=\"prev_flight_delay_ind\", hue=\"DEP_DEL15\", palette=\"coolwarm\")\n",
    "plt.title(\"Count of Delayed vs. Not Delayed Flights by Previous Delay Indicator (5 Years)\")\n",
    "plt.xlabel(\"Previous Flight Delayed (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Flight Count\")\n",
    "plt.legend(title=\"DEP_DEL15\", labels=[\"On-Time\", \"Delayed\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea4fd6d-669d-419c-bc87-144ecfc48d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the delay counts with percentage\n",
    "delay_counts = df_sample.groupby(\"prev_flight_delay_ind\")[\"DEP_DEL15\"].value_counts(normalize=True).unstack()\n",
    "\n",
    "# Show percentage table\n",
    "print(\"Percentage Table:\")\n",
    "print(delay_counts * 100)  # Multiplying by 100 to get the percentages\n",
    "\n",
    "# Create stacked bar chart\n",
    "ax = delay_counts.plot(kind=\"bar\", stacked=True, figsize=(8, 6))\n",
    "\n",
    "# Annotate percentages on the bars\n",
    "for p in ax.patches:\n",
    "    # Get the width and height of each bar segment\n",
    "    height = p.get_height()\n",
    "    width = p.get_width()\n",
    "    x, y = p.get_xy()  # Get the x and y position of the bar segment\n",
    "\n",
    "    # Calculate the percentage of each segment\n",
    "    percentage = height * 100  # Since the heights represent the percentages\n",
    "\n",
    "    # Annotate percentage on each bar segment\n",
    "    ax.annotate(f'{percentage:.1f}%', (x + width / 2, y + height / 2), ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Set the plot title and labels\n",
    "plt.title(\"Percentage of Delays by Previous Flight Delay Indicator (5 Years)\")\n",
    "plt.xlabel(\"Previous Flight Delayed (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.legend(title=\"DEP_DEL15\", labels=[\"On-Time\", \"Delayed\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2762804a-fa58-47cf-9299-6b8ddce62f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5aeb881-d307-4561-aec1-409b82930554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Holiday vs Delay Rate (DEP_DEL15)\n",
    "holiday_delay_pd = df_otpw_60m.groupBy(\"is_holiday\").agg(\n",
    "    F.count(\"*\").alias(\"total_flights\"),\n",
    "    F.sum(\"DEP_DEL15\").alias(\"num_delayed\"),\n",
    "    (F.sum(\"DEP_DEL15\") / F.count(\"*\")).alias(\"delay_rate\")\n",
    ").toPandas()\n",
    "\n",
    "# Plot Delay Rates by Holiday Indicator\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.barplot(data=holiday_delay_pd, x=\"is_holiday\", y=\"delay_rate\", palette=\"Set2\")\n",
    "\n",
    "# Add percentage labels on the bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{height * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2., height), \n",
    "                ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "plt.xticks([0, 1], [\"Non-Holiday\", \"Holiday (±2 Days)\"])\n",
    "plt.ylabel(\"Delay Rate\")\n",
    "plt.title(\"Flight Delay Rate by Holiday Indicator (5 Year)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f7f8708-3b32-4ed9-98dd-45ae4cba1b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d639a318-4f3c-4e67-8989-70d89da154d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "df_sample_60m = df_otpw_60m.select([\n",
    "     'AIR_TIME', 'DISTANCE', \n",
    "     'origin_station_lat', 'origin_station_lon', 'origin_airport_lat', 'origin_airport_lon', \n",
    "     'origin_station_dis', 'dest_station_lat', 'dest_station_lon', 'dest_airport_lat', 'dest_airport_lon', \n",
    "     'dest_station_dis', 'LATITUDE', 'LONGITUDE', 'ELEVATION', \n",
    "     'HourlyAltimeterSetting', 'HourlyDewPointTemperature', 'HourlyDryBulbTemperature', \n",
    "     'HourlyPrecipitation', 'HourlyPressureTendency', 'HourlyRelativeHumidity', \n",
    "     'HourlySeaLevelPressure', 'HourlyStationPressure', 'HourlyVisibility', 'HourlyWetBulbTemperature', \n",
    "     'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed', \n",
    "     'origin_normalized_out_degree', 'dest_normalized_in_degree', \n",
    "     'origin_pagerank', 'dest_pagerank'\n",
    "]).toPandas()\n",
    "\n",
    "# Compute Spearman correlation\n",
    "corr_matrix_spearman = df_sample_60m.corr(method=\"spearman\")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(corr_matrix_spearman, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, annot_kws={\"size\": 8})\n",
    "plt.title(\"Spearman Correlation Heatmap of Flight Delay & All Continuous Features (5 Years)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a63510-1fcd-4666-a3fa-663f1604edde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Plot histograms for numerical variables 60 month data\n",
    "axes = df_sample_60m.hist(figsize=(15,12), bins=30, edgecolor=\"black\")\n",
    "plt.suptitle(\"Numerical Feature Distributions (60m)\", fontsize=16)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    if ax.get_title(): \n",
    "        ax.set_title(ax.get_title(), fontsize=10) \n",
    "\n",
    "plt.subplots_adjust(wspace=0.8, hspace=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e2b936-991c-49f8-a58c-923abe38ab48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5574fd88-eb1a-40f7-b11d-4ccd0e6f3e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "# Drop Highly Correlated & Redundent Features\n",
    "drop_col = [\"DEP_DELAY_DOUBLE\", \"DEP_DELAY_NEW_DOUBLE\", \"DEP_DELAY\", \"DISTANCE\", \"ELEVATION\", \"HourlyAltimeterSetting\", \"HourlyDewPointTemperature\", \"HourlyWetBulbTemperature\", \"origin_station_lat\", \"origin_station_lon\", \"origin_airport_lat\", \"origin_airport_lon\", \"dest_station_lat\", \"dest_station_lon\", \"dest_normalized_in_degree\", \"origin_normalized_out_degree\"]\n",
    "\n",
    "# Drop Highly Correlated Features for 60 months data\n",
    "df_otpw_60m_cleaned = df_otpw_60m.drop(*drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6751ff29-d660-43be-b202-8cc2105b6bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read new 12 month data\n",
    "df_otpw_60m_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883ea4a4-611c-49ef-bb14-51b0a8ad278f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assess multicollinearity after dropping correlated features for 60 month data\n",
    "df_numeric_60m = df_otpw_60m_cleaned.select([\n",
    "     'origin_station_dis', 'dest_airport_lat', 'dest_airport_lon', 'dest_station_dis', 'LATITUDE', 'LONGITUDE', 'HourlyPressureTendency', 'HourlyDryBulbTemperature', 'HourlyPrecipitation', 'HourlyRelativeHumidity', 'HourlySeaLevelPressure', 'HourlyStationPressure', 'HourlyVisibility', 'HourlyWindDirection', 'HourlyWindGustSpeed', 'HourlyWindSpeed', 'origin_pagerank', 'dest_pagerank'\n",
    "]).toPandas()\n",
    "\n",
    "# # Compute correlation matrix\n",
    "corr_matrix = df_numeric_60m.corr(method=\"spearman\")\n",
    "\n",
    "# # Plot heatmap\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Spearman Correlation Heatmap of Continuous Features (5 Years)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e6bb87-8b88-4e6d-a8f0-d5e0ca2fcf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# LASSO Regularization - 60 month data\n",
    "\n",
    "# List of numerical features\n",
    "num_col = [c for c, t in df_otpw_60m_cleaned.dtypes if t in ('double') and c != \"DEP_DELAY_NEW\"]\n",
    "\n",
    "# Filter out rows with null values in the target column\n",
    "df_otpw_60m_cleaned = df_otpw_60m_cleaned.filter(df_otpw_60m_cleaned[\"DEP_DEL15\"].isNotNull())\n",
    "\n",
    "# create an assembler for numerical features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=num_col,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Lasso Regression (L1 Regularization)\n",
    "logReg_lasso = LogisticRegression(featuresCol=\"features\", labelCol=\"DEP_DEL15\", elasticNetParam=1.0)\n",
    "\n",
    "# Create a pipeline with indexers, encoders, assembler, and model\n",
    "pipeline = Pipeline(stages=[assembler, logReg_lasso])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(df_otpw_60m_cleaned)\n",
    "\n",
    "# Extract the coefficients of the model\n",
    "coef = model.stages[-1].coefficients\n",
    "intercept = model.stages[-1].intercept\n",
    "\n",
    "# Get the feature names from the assembler\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "# Find dropped and selected features - drop features with coefficients less than 0.001\n",
    "dropped_features = [(feature_names[i], coef[i]) for i in range(len(coef)) if abs(coef[i]) < 0.001]\n",
    "selected_features = [(feature_names[i], coef[i]) for i in range(len(coef)) if abs(coef[i]) >= 0.001]\n",
    "\n",
    "print(\"Dropped Features:\", dropped_features)\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8450ddb3-a317-44dd-902e-df53bd8c3db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define numerical features for PCA (using the same LASSO-selected features)\n",
    "feature_columns_60m = [f[0] for f in selected_features]  # Use the same selected features\n",
    "\n",
    "# Assemble the feature columns into a feature vector\n",
    "assembler_60m = VectorAssembler(inputCols=feature_columns_60m, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "# Standardize the features\n",
    "scaler_60m = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Apply PCA with 18 components \n",
    "k_60m = min(len(feature_columns_60m), 18)  \n",
    "pca_60m = PCA(k=k_60m, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Create a pipeline\n",
    "pca_pipeline_60m = Pipeline(stages=[assembler_60m, scaler_60m, pca_60m])\n",
    "\n",
    "# Fit the PCA model\n",
    "pca_model_60m = pca_pipeline_60m.fit(df_otpw_60m_cleaned.dropna(subset=feature_columns_60m))\n",
    "\n",
    "# Extract explained variance\n",
    "explained_variance_60m = pca_model_60m.stages[-1].explainedVariance.toArray()\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance_60m = np.cumsum(explained_variance_60m)\n",
    "\n",
    "# Scree Plot for 60-month data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, k_60m + 1), explained_variance_60m, marker='o', linestyle='--', label=\"Explained Variance\")\n",
    "plt.plot(range(1, k_60m + 1), cumulative_variance_60m, marker='s', linestyle='-', label=\"Cumulative Variance\", color='r')\n",
    "plt.axhline(y=0.85, color='g', linestyle='--', label=\"85% Variance Threshold\") \n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.title(\"Scree Plot - PCA (5 Years)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance\n",
    "print(\"Explained Variance by Principal Components (5 Years):\", explained_variance_60m)\n",
    "print(\"Cumulative Explained Variance (5 Years):\", cumulative_variance_60m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1d43e9-dce2-4512-97d0-bde829e6e9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop the overlapping columns (show dropped in 60m data) result from the LASSO regularization\n",
    "# Dropped Features: [('AIR_TIME', 0.0007176901231048505), ('origin_station_dis', 9.369368015092871e-06), ('dest_station_dis', -8.989046179898635e-05), ('HourlyWindDirection', 0.00032700957757733114)]\n",
    "\n",
    "drop_col_num = ['AIR_TIME', 'origin_station_dis', 'dest_station_dis', 'HourlyWindDirection']\n",
    "\n",
    "# Drop these columns\n",
    "df_otpw_60m_cleaned = df_otpw_60m_cleaned.drop(*drop_col_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3398872-4c29-412e-8009-a79186741eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get all string-type columns for 60m data\n",
    "string_columns = [field.name for field in df_otpw_60m_cleaned.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "# Compute unique value counts for string-type columns\n",
    "unique_counts = {col_name: df_otpw_60m_cleaned.select(col_name).distinct().count() for col_name in string_columns}\n",
    "\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea3994f-8c71-401e-96ed-5f4818337ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop categorical columns that are either duplicates or have too many unique values (high cardinality)\n",
    "drop_col_cat = ['ORIGIN_STATE_FIPS','ORIGIN_WAC',\"DEST_STATE_FIPS\",\"DEST_WAC\",'ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID', \n",
    "                'OP_CARRIER_FL_NUM', 'TAIL_NUM','STATION','HourlySkyConditions']\n",
    "\n",
    "# Drop these columns\n",
    "df_otpw_60m_cleaned = df_otpw_60m_cleaned.drop(*drop_col_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a87c75-3e15-4784-8f9f-21279d144814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_columns = [field.name for field in df_otpw_60m_cleaned.schema.fields if isinstance(field.dataType, StringType)]\n",
    "string_columns\n",
    "\n",
    "# Compute unique value counts for string-type columns\n",
    "unique_counts = {col_name: df_otpw_60m_cleaned.select(col_name).distinct().count() for col_name in string_columns}\n",
    "\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1abe8b6-ddc9-4d5d-9a67-598bc8394c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m_cleaned.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a8803b-b858-4ce3-9051-a4d0f6b52009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final removed columns\n",
    "drop_col_final = ['DATE', 'two_hours_prior_depart_UTC', 'four_hours_prior_depart_UTC','DEP_DELAY_NEW','LATITUDE','LONGITUDE','ORIGIN_CITY_MARKET_ID', 'DEST_CITY_MARKET_ID']\n",
    "\n",
    "# Drop these columns\n",
    "df_otpw_60m_cleaned = df_otpw_60m_cleaned.drop(*drop_col_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9b9708-a49e-47c2-b44e-350faadba43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60m_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941e2dcb-2995-42f2-8e1e-28ec9e6b7883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final Review\n",
    "df_otpw_60m_cleaned.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c57f43-19e9-475f-8e84-3bce52b7b56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checkpoint the data after dimensionality reduction\n",
    "df_otpw_60m_cleaned.write.mode(\"overwrite\").parquet(f\"{folder_path}/otpw_60m_proceed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b89a7a-d48f-47f7-b7d6-e6ac78293f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Train-Test Split on 5 Years Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42890f17-6ae7-49d2-ac57-6637b4230542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read datasets\n",
    "section = \"02\"\n",
    "number = \"01\"\n",
    "folder_path = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "\n",
    "df_otpw_60m = spark.read.parquet(f\"{folder_path}/otpw_60m_proceed.parquet\")\n",
    "print(\"Size of df_otpw_60m:\", df_otpw_60m.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78babc88-b6c8-49a2-ad06-29eb9711487c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(df_otpw_60m.filter(F.col(\"HourlyPrecipitation\").isNull()).count())\n",
    "print(df_otpw_60m.filter(F.col(\"HourlySeaLevelPressure\").isNull()).count())\n",
    "print(df_otpw_60m.filter(F.col(\"HourlyWindGustSpeed\").isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f72158-be43-49e9-9833-3d0a77f4760d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates in df_otpw_60m dataset\n",
    "df_otpw_60m = df_otpw_60m.dropDuplicates()\n",
    "\n",
    "# print the size of df_otpw_60m dataset\n",
    "print(\"Size of df_otpw_60m:\", df_otpw_60m.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6196f8a4-edb1-4c11-8aad-10ab7af2f9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand,col,when,concat,substring,lit,udf,lower,sum as ps_sum,count as ps_count,row_number\n",
    "\n",
    "# Remove null values in the target variable\n",
    "df_otpw_60m = df_otpw_60m.filter(F.col(\"DEP_DEL15\").isNotNull())\n",
    "\n",
    "# Train-Test Split for 60-Month Dataset\n",
    "df_train = df_otpw_60m.filter(col(\"sched_depart_date_time_UTC\") < \"2019-01-01 00:00:00\")  # 2015-2018\n",
    "df_test = df_otpw_60m.filter(col(\"sched_depart_date_time_UTC\") >= \"2019-01-01 00:00:00\")  # 2019 (blind test)\n",
    "\n",
    "print(\"Training Set Size (2015–2018):\", df_train.count())\n",
    "print(\"Test Set Size (2019):\", df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf98e75-53f6-417c-bd93-8653c4889758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "df_train_pandas = df_train.toPandas()\n",
    "df_test_pandas = df_test.toPandas()\n",
    "\n",
    "# Create a new column for the dataset split (train or test) in both dataframes\n",
    "df_train_pandas['Dataset'] = 'Train'\n",
    "df_test_pandas['Dataset'] = 'Test'\n",
    "\n",
    "# Concatenate the two datasets to combine them for plotting\n",
    "df_combined = pd.concat([df_train_pandas[['DEP_DEL15', 'Dataset']], df_test_pandas[['DEP_DEL15', 'Dataset']]])\n",
    "\n",
    "# Replace 0 with 'On Time' and 1 with 'Delayed' for better labeling\n",
    "df_combined['DEP_DEL15'] = df_combined['DEP_DEL15'].replace({0: 'On Time', 1: 'Delayed'})\n",
    "\n",
    "# Create a stacked bar plot to compare the distribution of On Time vs Delayed in both sets\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='DEP_DEL15', hue='Dataset', data=df_combined, palette='Set2')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(\"Comparison of On Time vs Delayed Flights in Train and Test Sets (5 Years)\")\n",
    "plt.xlabel(\"Flight Status\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title='Dataset', loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a368d7-fe6e-4cc4-8c70-f351feaf41f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5713c8b3-865f-4cb6-91ee-f3e8e3bfacff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#downsampling \n",
    "def downsample(train_df,verbose=False):\n",
    "  '''Downsamples train_df to balance classes'''\n",
    "  #balance classes in train\n",
    "  delay_count = train_df.filter(F.col(\"DEP_DEL15\") == 1).count()\n",
    "  non_delay_count = train_df.filter(F.col(\"DEP_DEL15\") == 0).count()\n",
    " \n",
    "  total = delay_count + non_delay_count\n",
    "  keep_percent = delay_count / non_delay_count\n",
    "  \n",
    "  train_delay = train_df.filter(F.col('DEP_DEL15') == 1)\n",
    "  train_non_delay = train_df.filter(F.col('DEP_DEL15') == 0).sample(withReplacement=False,fraction=keep_percent,seed=42)\n",
    "  train_downsampled = train_delay.union(train_non_delay)\n",
    "  return train_downsampled  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b706c268-b383-49ae-b0ed-38434171fd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Downsample the training data from the 60-month dataset\n",
    "df_train_downsampled = downsample(df_train)\n",
    "df_train_downsampled.groupBy(\"DEP_DEL15\").count().show()\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "df_train_downsampled_pandas = df_train_downsampled.toPandas()\n",
    "df_test_pandas = df_test.toPandas()\n",
    "\n",
    "# Create a new column for the dataset split (train or test) in both dataframes\n",
    "df_train_downsampled_pandas['Dataset'] = 'Train'\n",
    "df_test_pandas['Dataset'] = 'Test'\n",
    "\n",
    "# Concatenate the two datasets to combine them for plotting\n",
    "df_combined_60m_downsampled = pd.concat([df_train_downsampled_pandas[['DEP_DEL15', 'Dataset']], df_test_pandas[['DEP_DEL15', 'Dataset']]])\n",
    "\n",
    "# Replace 0 with 'On Time' and 1 with 'Delayed' for better labeling\n",
    "df_combined_60m_downsampled['DEP_DEL15'] = df_combined_60m_downsampled['DEP_DEL15'].replace({0: 'On Time', 1: 'Delayed'})\n",
    "\n",
    "# Create a stacked bar plot to compare the distribution of On Time vs Delayed in both sets after downsampling\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='DEP_DEL15', hue='Dataset', data=df_combined_60m_downsampled, palette='Set2', order=['On Time', 'Delayed'])\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(\"Comparison of On Time vs Delayed Flights in Train and Test Sets (5 Years Data) After Downsampling\")\n",
    "plt.xlabel(\"Flight Status\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title='Dataset', loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count the occurrences of 0 and 1 for both train and test sets after downsampling\n",
    "count_table_downsampled = df_combined_60m_downsampled.groupby(['Dataset', 'DEP_DEL15']).size().unstack(fill_value=0)\n",
    "\n",
    "# Display the count table for the downsampled dataset\n",
    "print(\"Count of On Time and Delayed flights in Train and Test sets (After Downsampling):\")\n",
    "print(count_table_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef80f20-85a7-4aab-992b-47b5a8f8d020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Point Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c0f11d-0f35-4a3a-95f9-fcf65115817c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the original train set (after downsampling) and test set as parquet files\n",
    "df_train.write.mode(\"overwrite\").parquet(f\"{folder_path}/df_train.parquet\")\n",
    "df_train_downsampled.write.mode(\"overwrite\").parquet(f\"{folder_path}/df_train_downsampled.parquet\")\n",
    "df_test.write.mode(\"overwrite\").parquet(f\"{folder_path}/df_test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "848a23e1-4a38-4b97-b84c-de53113ea481",
     "origId": 1536813829684474,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "w261_FP_Phase_3_Data Preprocessing_EDA_Team_2_1_Code_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}